/**
 * Ollama Provider Implementation (TASK A ìˆ˜ì • ë²„ì „)
 * Ollama APIë¥¼ í†µí•œ LLM ìš”ì²­ ì²˜ë¦¬ - ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©
 */

import { v4 as uuidv4 } from 'uuid';
import { BaseLLMProvider } from './base.provider';
import { ChatRequest, ChatResponse, ModelInfo, LLMProvider, ModelCapability } from '../types/contracts';
import { LLMProvider as ProviderType } from '../types/provider.types';
import { finalConfig } from '../config';
import logger from '../utils/logger';

interface OllamaMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OllamaChatRequest {
  model: string;
  messages: OllamaMessage[];
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    num_predict?: number;
    stop?: string[];
  };
}

interface OllamaChatResponse {
  model: string;
  created_at: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export class OllamaProvider extends BaseLLMProvider {
  protected providerType: LLMProvider = 'ollama';
  protected apiKey: string = ''; // OllamaëŠ” API í‚¤ê°€ í•„ìš” ì—†ìŒ
  private baseUrl: string;

  constructor(config?: ProviderType) {
    super();
    
    if (config?.config?.baseUrl) {
      // ğŸ”¥ TASK A ìˆ˜ì •: MongoDBì—ì„œ ê°€ì ¸ì˜¨ ë™ì  ì„¤ì • ìš°ì„  ì‚¬ìš©
      this.baseUrl = config.config.baseUrl;
      logger.info(`ğŸ”¥ [TASK-A] Ollama provider using dynamic config: ${this.baseUrl}`);
    } else {
      // í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ í´ë°± (ìµœí›„ ìˆ˜ë‹¨)
      const providerConfig = finalConfig.providers.find(p => p.providerType === 'ollama');
      this.baseUrl = providerConfig?.config?.baseUrl || 'http://localhost:11434';
      logger.warn(`âš ï¸ [TASK-A] Ollama provider using fallback config: ${this.baseUrl}`);
    }

    logger.info(`âœ… [TASK-A] Ollama provider initialized with baseUrl: ${this.baseUrl}`);
  }

  /**
   * ğŸ”¥ TASK A ìˆ˜ì •: ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ - ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©
   */
  async chat(request: ChatRequest): Promise<ChatResponse> {
    this.validateRequest(request);

    try {
      // ğŸ”¥ ìˆ˜ì •: ë™ì  ëª¨ë¸ ì„ íƒ - ì§€ì •ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì‚¬ìš©
      let modelToUse = request.model;
      
      if (!modelToUse) {
        try {
          const availableModels = await this.getModels();
          if (availableModels.length > 0) {
            modelToUse = availableModels[0].id;
            logger.info(`ğŸ”„ [TASK-A] Auto-selected first available model: ${modelToUse}`);
          } else {
            throw new Error('No models available');
          }
        } catch (error) {
          logger.error('âŒ Failed to get available models for auto-selection:', error);
          throw new Error('No model specified and cannot determine available models');
        }
      }
      
      const ollamaRequest: OllamaChatRequest = {
        model: modelToUse,
        messages: request.messages.map(msg => ({
          role: msg.role,
          content: msg.content,
        })),
        stream: false,
        options: {
          temperature: request.temperature,
          top_p: request.topP,
          num_predict: request.maxTokens,
          stop: request.stop,
        },
      };

      logger.info('ğŸš€ Sending Ollama chat request:', {
        model: ollamaRequest.model,
        messageCount: ollamaRequest.messages.length,
        temperature: ollamaRequest.options?.temperature,
        lastMessage: ollamaRequest.messages.slice(-1)[0]?.content?.substring(0, 50) + '...',
      });

      // Node.js http ëª¨ë“ˆ ì‚¬ìš©
      const response = await this.makeHttpRequest('/api/chat', 'POST', ollamaRequest);
      const ollamaResponse = response as OllamaChatResponse;

      if (!ollamaResponse.message?.content) {
        throw new Error('No response content from Ollama');
      }

      // í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚° (Ollama ì‘ë‹µì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ)
      const promptTokens = ollamaResponse.prompt_eval_count || this.estimateTokens(
        request.messages.map(m => m.content).join(' ')
      );
      const completionTokens = ollamaResponse.eval_count || this.estimateTokens(
        ollamaResponse.message.content
      );

      const chatResponse: ChatResponse = {
        id: uuidv4(),
        model: ollamaResponse.model,
        usage: {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens,
          cost: 0, // OllamaëŠ” ë¬´ë£Œ
        },
        choices: [{
          message: {
            role: 'assistant',
            content: ollamaResponse.message.content,
          },
          finishReason: ollamaResponse.done ? 'stop' : 'length',
        }],
        createdAt: new Date().toISOString(),
        finishReason: ollamaResponse.done ? 'stop' : 'length',
      };

      logger.info('âœ… Ollama chat response received:', {
        model: chatResponse.model,
        promptTokens,
        completionTokens,
        contentLength: ollamaResponse.message.content.length,
        contentPreview: ollamaResponse.message.content.substring(0, 100) + '...',
      });

      return chatResponse;

    } catch (error: any) {
      logger.error('âŒ Ollama API error:', {
        error: error instanceof Error ? error.message : 'Unknown error',
        baseUrl: this.baseUrl,
        model: request.model,
      });
      
      throw new Error(`Ollama request failed: ${error.message || 'Unknown error'}`);
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ì¶”í›„ êµ¬í˜„)
   */
  async* streamChat(request: ChatRequest): AsyncGenerator<string> {
    // TODO: Ollama ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
    logger.warn('Ollama streaming not implemented yet, falling back to regular chat');
    const response = await this.chat(request);
    yield response.choices[0]?.message.content || '';
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async getModels(): Promise<ModelInfo[]> {
    try {
      logger.info('Fetching Ollama models from:', `${this.baseUrl}/api/tags`);
      
      const response = await this.makeHttpRequest('/api/tags', 'GET');
      const data = response as { models: any[] };

      const models: ModelInfo[] = data.models.map((model: any) => ({
        id: model.name,
        name: model.name,
        provider: 'ollama' as LLMProvider,
        capabilities: ['chat'] as ModelCapability[],
        contextWindow: this.getContextWindowForModel(model.name),
        costPer1kTokens: {
          input: 0,
          output: 0,
        },
        status: 'active' as const,
        description: `Ollama model: ${model.name}`,
        version: model.digest ? model.digest.substring(0, 12) : 'unknown',
      }));

      logger.info(`Found ${models.length} Ollama models:`, models.map(m => m.name));
      return models;

    } catch (error: any) {
      logger.error('Failed to get Ollama models:', {
        error: error instanceof Error ? error.message : 'Unknown error',
        baseUrl: this.baseUrl,
      });
      
      // ğŸ”¥ ìˆ˜ì •: ê¸°ë³¸ ëª¨ë¸ë“¤ì„ ë°˜í™˜ (ì˜¬ë°”ë¥¸ ëª¨ë¸ëª…)
      return [
        {
          id: 'gemma3:12b',
          name: 'Gemma 3 12B',
          provider: 'ollama',
          capabilities: ['chat'],
          contextWindow: 8192,
          costPer1kTokens: { input: 0, output: 0 },
          status: 'active',
          description: 'Gemma 3 12B model for production use',
        },
        {
          id: 'qwen3:14b',
          name: 'Qwen 3 14B',
          provider: 'ollama',
          capabilities: ['chat'],
          contextWindow: 32768,
          costPer1kTokens: { input: 0, output: 0 },
          status: 'active',
          description: 'Qwen 3 14B model for advanced tasks',
        }
      ];
    }
  }

  /**
   * í—¬ìŠ¤ì²´í¬
   */
  async healthCheck(): Promise<boolean> {
    try {
      logger.info('Performing Ollama health check:', `${this.baseUrl}/api/tags`);
      
      await this.makeHttpRequest('/api/tags', 'GET');
      
      logger.info('âœ… Ollama health check passed');
      return true;
    } catch (error: any) {
      logger.error('âŒ Ollama health check failed:', {
        error: error instanceof Error ? error.message : 'Unknown error',
        baseUrl: this.baseUrl,
      });
      return false;
    }
  }

  /**
   * HTTP ìš”ì²­ì„ ë§Œë“œëŠ” í—¬í¼ ë©”ì„œë“œ (axios ëŒ€ì‹  Node.js http ì‚¬ìš©)
   */
  private async makeHttpRequest(path: string, method: 'GET' | 'POST', data?: any): Promise<any> {
    const url = new URL(path, this.baseUrl);
    
    const options = {
      method,
      headers: {
        'Content-Type': 'application/json',
      },
      timeout: 30000,
    };

    const body = data ? JSON.stringify(data) : undefined;

    return new Promise((resolve, reject) => {
      const protocol = url.protocol === 'https:' ? require('https') : require('http');
      
      const req = protocol.request(url, options, (res: any) => {
        let responseData = '';
        
        res.on('data', (chunk: any) => {
          responseData += chunk;
        });
        
        res.on('end', () => {
          try {
            if (res.statusCode >= 200 && res.statusCode < 300) {
              const parsed = JSON.parse(responseData);
              resolve(parsed);
            } else {
              reject(new Error(`HTTP ${res.statusCode}: ${responseData}`));
            }
          } catch (error) {
            reject(new Error(`Failed to parse response: ${responseData}`));
          }
        });
      });

      req.on('error', (error: any) => {
        reject(error);
      });

      req.on('timeout', () => {
        req.destroy();
        reject(new Error('Request timeout'));
      });

      if (body) {
        req.write(body);
      }
      
      req.end();
    });
  }

  /**
   * ğŸ”¥ ìˆ˜ì •: ëª¨ë¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ì¶”ì • (gemma3 ìš°ì„ )
   */
  private getContextWindowForModel(modelName: string): number {
    // ëª¨ë¸ëª…ì— ë”°ë¥¸ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ì¶”ì •
    if (modelName.includes('gemma3')) {
      return 8192;
    }
    if (modelName.includes('gemma2')) {
      return 8192;
    }
    if (modelName.includes('llama')) {
      return 4096;
    }
    if (modelName.includes('qwen3')) {
      return 32768;
    }
    if (modelName.includes('qwen')) {
      return 32768;
    }
    return 4096; // ê¸°ë³¸ê°’
  }

  /**
   * í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (OllamaëŠ” ì •í™•í•œ í† í° ì¹´ìš´íŠ¸ë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
   */
  protected estimateTokens(text: string): number {
    // ë” ì •í™•í•œ í† í° ì¶”ì • (í•œêµ­ì–´ ê³ ë ¤)
    // í•œêµ­ì–´ëŠ” ë³´í†µ 1.5-2 ê¸€ìë‹¹ 1í† í°
    const koreanChars = (text.match(/[ê°€-í£]/g) || []).length;
    const englishWords = (text.match(/[a-zA-Z]+/g) || []).length;
    const otherChars = text.length - koreanChars - englishWords;
    
    return Math.ceil(koreanChars / 1.5 + englishWords * 1.3 + otherChars / 4);
  }
}
